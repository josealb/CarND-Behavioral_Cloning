{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training driving model with behavioral cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we will train a NN using samples obtained from Udacity Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from shutil import copyfile\n",
    "import pandas\n",
    "import fileinput\n",
    "#Create directory with all data merged into one\n",
    "datafolder = '/media/josealb/HDD_1/Datasets/Self_driving/Simulator_Data/Merge'\n",
    "outputfolder = '/media/josealb/HDD_1/Datasets/Self_driving/Simulator_Data/All_tracks'\n",
    "subdirs = [x[0] for x in os.walk(datafolder)] \n",
    "subdirs.pop(0)\n",
    "subdirs\n",
    "fileList=[]\n",
    "os.mkdir(outputfolder+'/IMG') \n",
    "\n",
    "\n",
    "for dir in subdirs:\n",
    "    if \"IMG\" in dir:\n",
    "        print(dir)\n",
    "        files= [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "        for file in files:\n",
    "            copyfile(dir+'/'+file,outputfolder+'/IMG/'+file)\n",
    "    if \"IMG\" not in dir:\n",
    "        print(dir)\n",
    "        fileList.append(dir+'/driving_log.csv')\n",
    "        \n",
    "dfList=[]\n",
    "for filename in fileList:\n",
    "    print(filename)\n",
    "    df=pandas.read_csv(filename,header=None)\n",
    "    dfList.append(df)\n",
    "concatDf=pandas.concat(dfList,axis=0)\n",
    "concatDf.to_csv(outputfolder+'/driving_log.csv',index=None,header=None)\n",
    "\n",
    "with fileinput.FileInput(outputfolder+'/driving_log.csv', inplace=True, backup='.bak') as file:\n",
    "    for line in file:\n",
    "        print(line.replace('\\\\', '/'), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Lambda\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers.convolutional import Convolution2D, Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "datafolder = '/media/josealb/HDD_1/Datasets/Self_driving/Simulator_Data/All_tracks/'\n",
    "#datafolder = '/media/josealb/HDD_1/Datasets/Self_driving/Simulator_Data/Merge/ND_Sample/'\n",
    "samples = []\n",
    "with open(datafolder+'driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name_center = datafolder+'IMG/'+batch_sample[0].split('/')[-1]\n",
    "                name_left = datafolder+'IMG/'+batch_sample[1].split('/')[-1]\n",
    "                name_right = datafolder+'IMG/'+batch_sample[2].split('/')[-1]\n",
    "\n",
    "                #name = datafolder+'IMG/'+batch_sample[0].split('\\\\')[-1]\n",
    "\n",
    "                center_image = cv2.imread(name_center)\n",
    "                left_image = cv2.imread(name_left)\n",
    "                right_image = cv2.imread(name_right)\n",
    "                \n",
    "                if center_image is None:\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                center_angle = float(batch_sample[3])\n",
    "                center_angle = center_angle*2 #Makes Neural network turn more aggresively\n",
    "                correction = 0.25\n",
    "                \n",
    "                left_angle = center_angle + correction #Udacity has + here\n",
    "                right_angle= center_angle - correction\n",
    "\n",
    "                images.append(center_image) #should be made random\n",
    "                angles.append(center_angle)\n",
    "                #images.append(left_image)\n",
    "                #angles.append(left_angle)\n",
    "                #images.append(right_image)\n",
    "                #angles.append(right_angle)     \n",
    "                   \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=512)\n",
    "validation_generator = generator(validation_samples, batch_size=512)\n",
    "\n",
    "#ch, row, col = 3, 80, 320  # Trimmed image format\n",
    "ch, row, col = 3, 160, 320  # UnTrimmed image format\n",
    "#ELU instead of RELU?\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(row,col,ch)))\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.))#,\n",
    "       # input_shape=(row, col, ch),\n",
    "        #output_shape=(row, col, ch)))\n",
    "#model.add(Conv2D(24,5,5,subsample=(2,2), activation=\"elu\"))\n",
    "#model.add(Convolution2D(36,5,5,subsample=(2,2), activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Convolution2D(48,5,5,subsample=(2,2), activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Convolution2D(64,3,3, activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Convolution2D(64,3,3, activation=\"elu\"))\n",
    "##model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1164))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(100))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(50))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(10))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(24,9,9, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(24,5,5,subsample=(2,2), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(48,5,5,subsample=(2,2), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64,3,3,subsample=(2,2), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64,3,3,subsample=(2,2), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{val_loss:03f}.h5',\n",
    "                            monitor='val_loss',\n",
    "                            verbose=0,\n",
    "                            save_best_only=True,\n",
    "                            mode = 'auto')\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples)*3, validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples), nb_epoch=3, callbacks= [checkpoint], verbose=1)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples), validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples), nb_epoch=10, callbacks= [checkpoint], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
