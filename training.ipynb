{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training driving model with behavioral cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we will train a NN using samples obtained from Udacity Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "19284/19284 [==============================] - 26s - loss: 1.9723 - val_loss: 0.0333\n",
      "Epoch 2/3\n",
      "19284/19284 [==============================] - 23s - loss: 0.1941 - val_loss: 0.0303\n",
      "Epoch 3/3\n",
      "19284/19284 [==============================] - 23s - loss: 0.1202 - val_loss: 0.0335\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Lambda\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers.convolutional import Convolution2D, Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "datafolder = '/media/josealb/HDD_1/Datasets/Self_driving/Simulator_Data/ND_Sample/'\n",
    "#datafolder = '/media/josealb/HDD_1/Datasets/Self_driving/Simulator_Data/Track_1/'\n",
    "samples = []\n",
    "with open(datafolder+'driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name_center = datafolder+'IMG/'+batch_sample[0].split('/')[-1]\n",
    "                name_left = datafolder+'IMG/'+batch_sample[1].split('/')[-1]\n",
    "                name_right = datafolder+'IMG/'+batch_sample[2].split('/')[-1]\n",
    "\n",
    "                #name = datafolder+'IMG/'+batch_sample[0].split('\\\\')[-1]\n",
    "\n",
    "                center_image = cv2.imread(name_center)\n",
    "                left_image = cv2.imread(name_left)\n",
    "                right_image = cv2.imread(name_right)\n",
    "\n",
    "                center_angle = float(batch_sample[3])\n",
    "                center_angle = center_angle#Makes Neural network turn more aggresively\n",
    "                correction = 0.2\n",
    "                \n",
    "                left_angle = center_angle + correction\n",
    "                right_angle= center_angle - correction\n",
    "                \n",
    "                images.append(center_image) #should be made random\n",
    "                angles.append(center_angle)\n",
    "                \n",
    "                images.append(left_image)\n",
    "                angles.append(left_angle)\n",
    "                \n",
    "                images.append(right_image)\n",
    "                angles.append(right_angle)                \n",
    "\n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "#ch, row, col = 3, 80, 320  # Trimmed image format\n",
    "ch, row, col = 3, 160, 320  # UnTrimmed image format\n",
    "#ELU instead of RELU?\n",
    "\n",
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "\n",
    "model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(row,col,ch)))\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.))#,\n",
    "       # input_shape=(row, col, ch),\n",
    "        #output_shape=(row, col, ch)))\n",
    "#model.add(Convolution2D(24,9,9, activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Conv2D(24,5,5,subsample=(2,2), activation=\"elu\"))\n",
    "model.add(Convolution2D(36,5,5,subsample=(2,2), activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Convolution2D(48,5,5,subsample=(2,2), activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Convolution2D(64,3,3, activation=\"elu\"))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Convolution2D(64,3,3, activation=\"elu\"))\n",
    "##model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1164))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(100))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{val_loss:03f}.h5',\n",
    "                            monitor='val_loss',\n",
    "                            verbose=0,\n",
    "                            save_best_only=True,\n",
    "                            mode = 'auto')\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples)*3, validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples), nb_epoch=3, callbacks= [checkpoint], verbose=1)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6428/6428 [==============================] - 14s - loss: 0.0446 - val_loss: 0.0402\n",
      "Epoch 2/10\n",
      "6428/6428 [==============================] - 14s - loss: 0.0429 - val_loss: 0.0391\n",
      "Epoch 3/10\n",
      "6428/6428 [==============================] - 16s - loss: 0.0431 - val_loss: 0.0389\n",
      "Epoch 4/10\n",
      "6428/6428 [==============================] - 16s - loss: 0.0420 - val_loss: 0.0389\n",
      "Epoch 5/10\n",
      "6428/6428 [==============================] - 16s - loss: 0.0418 - val_loss: 0.0382\n",
      "Epoch 6/10\n",
      "6428/6428 [==============================] - 17s - loss: 0.0408 - val_loss: 0.0383\n",
      "Epoch 7/10\n",
      "6428/6428 [==============================] - 16s - loss: 0.0402 - val_loss: 0.0387\n",
      "Epoch 8/10\n",
      "6428/6428 [==============================] - 16s - loss: 0.0409 - val_loss: 0.0380\n",
      "Epoch 9/10\n",
      "6428/6428 [==============================] - 15s - loss: 0.0401 - val_loss: 0.0370\n",
      "Epoch 10/10\n",
      "6428/6428 [==============================] - 14s - loss: 0.0388 - val_loss: 0.0375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5a720cef28>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, samples_per_epoch= \\\n",
    "            len(train_samples), validation_data=validation_generator, \\\n",
    "            nb_val_samples=len(validation_samples), nb_epoch=10, callbacks= [checkpoint], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1febd54f7b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
